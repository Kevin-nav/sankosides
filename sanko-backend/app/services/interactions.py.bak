"""
Google Gemini Interactions API Client

This module provides a stateful client for the Gemini Interactions API.
Uses the google-genai SDK for proper server-side state management.

Key Features:
- Server-side state management (store=True)
- Multi-turn conversations via previous_interaction_id chaining
- Support for Deep Research agent (async background tasks)
- Native PDF/image processing via multimodal input
- Thinking level configuration for reasoning control

Reference: Interactions API Technical Guide Research.md
"""

from typing import Optional, Dict, Any, List, Union
import asyncio
import base64
from pathlib import Path

# Use google-genai SDK
from google import genai
from google.genai import types


class GeminiInteractionsClient:
    """
    Stateful client for Google Gemini Interactions API.
    
    This client manages interaction_id to maintain server-side conversation state,
    dramatically reducing token usage for multi-turn conversations.
    """
    
    def __init__(self, api_key: str):
        """
        Initialize the Gemini Interactions client.
        
        Args:
            api_key: Google Gemini API key
        """
        self.api_key = api_key
        self.client = genai.Client(api_key=api_key)
    
    def _extract_token_usage(self, response) -> Dict[str, int]:
        """
        Extract token usage from API response.
        
        Handles both response types:
        - generate_content: usage_metadata with prompt_token_count, candidates_token_count
        - interactions.create: usage with input_tokens, output_tokens, total_thought_tokens
        """
        tokens = {
            "input_tokens": 0,
            "output_tokens": 0,
            "thinking_tokens": 0,
            "cached_tokens": 0,
            "total_tokens": 0,
        }
        
        # Interactions API uses 'usage' attribute with 'total_*' prefixed fields
        if hasattr(response, 'usage') and response.usage:
            usage = response.usage
            tokens["input_tokens"] = getattr(usage, 'total_input_tokens', 0) or 0
            tokens["output_tokens"] = getattr(usage, 'total_output_tokens', 0) or 0
            tokens["thinking_tokens"] = getattr(usage, 'total_thought_tokens', 0) or 0
            tokens["cached_tokens"] = getattr(usage, 'cached_tokens', 0) or 0
        
        # generate_content uses 'usage_metadata' attribute
        elif hasattr(response, 'usage_metadata') and response.usage_metadata:
            meta = response.usage_metadata
            tokens["input_tokens"] = getattr(meta, 'prompt_token_count', 0) or 0
            tokens["output_tokens"] = getattr(meta, 'candidates_token_count', 0) or 0
            tokens["thinking_tokens"] = getattr(meta, 'thoughts_token_count', 0) or 0
            tokens["cached_tokens"] = getattr(meta, 'cached_content_token_count', 0) or 0
        
        # Fallback: check metadata dict
        elif hasattr(response, 'metadata') and response.metadata:
            meta = response.metadata
            if isinstance(meta, dict):
                tokens["input_tokens"] = meta.get('prompt_token_count', 0) or meta.get('input_tokens', 0)
                tokens["output_tokens"] = meta.get('candidates_token_count', 0) or meta.get('output_tokens', 0)
                tokens["thinking_tokens"] = meta.get('thoughts_token_count', 0) or meta.get('total_thought_tokens', 0)
        
        tokens["total_tokens"] = (
            tokens["input_tokens"] + 
            tokens["output_tokens"] + 
            tokens["thinking_tokens"]
        )
        
        return tokens
    
    async def create_interaction(
        self,
        prompt: Union[str, List[Dict]],
        model: str = "gemini-3-flash-preview",
        system_instruction: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        store: bool = True,
        previous_interaction_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Create a stateful interaction using the Interactions API.
        
        USE THIS FOR: Multi-turn conversations where you need session state
                      (e.g., clarification flow with user).
        
        NOTE: The beta Interactions API does NOT support thinking_config.
              For thinking-enabled calls, use generate_with_thinking() instead.
        
        Benefits:
        - Server-side state management via previous_interaction_id
        - KV cache reuse for faster subsequent turns
        - Real interaction IDs for session tracking
        
        Reference: Google AI Studio Gemini API Inquiry.md
        
        Args:
            prompt: The user's message (string or multimodal parts)
            model: Which Gemini model to use
            system_instruction: Optional system prompt
            tools: Optional list of tool definitions
            store: Whether to persist state server-side (default True)
            previous_interaction_id: For continuing a stateful session
            
        Returns:
            Dict containing:
            - interaction_id: Real session ID for subsequent turns
            - response: The model's response text
            - status: The interaction status
            - tokens: Token usage breakdown
        """
        try:
            # Run in thread pool since google-genai is sync
            loop = asyncio.get_event_loop()
            
            # Build kwargs for interactions.create
            # NOTE: NOT passing thinking_config - beta API doesn't support it
            create_kwargs = {
                "model": model,
                "input": prompt,
                "store": store,
            }
            
            # Add system instruction directly (not via generation_config)
            if system_instruction:
                create_kwargs["system_instruction"] = system_instruction
            
            # Add tools directly
            if tools:
                create_kwargs["tools"] = tools
            
            # Add previous_interaction_id for stateful continuation
            if previous_interaction_id:
                create_kwargs["previous_interaction_id"] = previous_interaction_id
            
            interaction = await loop.run_in_executor(
                None,
                lambda: self.client.interactions.create(**create_kwargs)
            )
            
            # Extract response text from outputs
            response_text = ""
            
            if hasattr(interaction, 'outputs') and interaction.outputs:
                for output in interaction.outputs:
                    # Text content
                    if hasattr(output, 'text') and output.text:
                        response_text += output.text
                    # Check for parts (some responses have nested structure)
                    if hasattr(output, 'parts'):
                        for part in output.parts:
                            if hasattr(part, 'text') and part.text:
                                response_text += part.text
            
            # Extract token usage from response metadata
            token_usage = self._extract_token_usage(interaction)
            
            # Get the interaction ID for stateful sessions
            interaction_id = getattr(interaction, 'id', None) or getattr(interaction, 'name', '')
            
            return {
                "interaction_id": interaction_id,  # Real ID from Interactions API!
                "response": response_text,
                "thinking": "",  # Not available in Interactions API beta
                "status": getattr(interaction, 'status', 'completed'),
                "tokens": token_usage,
                "raw": interaction,
            }
            
        except Exception as e:
            raise Exception(f"Failed to create interaction: {str(e)}")
    
    async def continue_interaction(
        self,
        interaction_id: str,
        prompt: Union[str, List[Dict]],
        model: str = "gemini-3-flash-preview",
        system_instruction: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
    ) -> Dict[str, Any]:
        """
        Continue an existing interaction with a new message.
        
        Uses the Interactions API with previous_interaction_id for true
        server-side stateful continuation. The server retrieves the full
        context and KV cache from the previous interaction automatically.
        
        NOTE: Thinking mode is NOT available in Interactions API beta.
              Use generate_with_thinking() for thinking-enabled calls.
        
        Reference: Google AI Studio Gemini API Inquiry.md, Section 3.3
        
        Args:
            interaction_id: The previous interaction's ID for state continuation
            prompt: The user's new message
            model: Which model to use for this turn
            system_instruction: Optional system prompt override
            tools: Optional tools override
            
        Returns:
            Dict containing:
            - interaction_id: New interaction ID (for next continuation)
            - response: The model's response text
            - status: The interaction status
            - tokens: Token usage breakdown
        """
        # Delegate to create_interaction with previous_interaction_id
        return await self.create_interaction(
            prompt=prompt,
            model=model,
            system_instruction=system_instruction,
            tools=tools,
            previous_interaction_id=interaction_id,  # Stateful continuation!
        )
    
    async def generate_with_thinking(
        self,
        prompt: Union[str, List[Dict]],
        model: str = "gemini-3-flash-preview",
        system_instruction: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        thinking_level: str = "high",
    ) -> Dict[str, Any]:
        """
        Generate content with thinking/reasoning mode enabled.
        
        USE THIS FOR: Complex reasoning tasks like planning, analysis, coding.
                      (e.g., Planner agent, Generator agent)
        
        NOTE: This uses models.generate_content (stateless) because the beta
              Interactions API does NOT support thinking_config.
        
        Reference: https://ai.google.dev/gemini-api/docs/thinking
        
        Args:
            prompt: The prompt (string or multimodal parts)
            model: Which Gemini model to use
            system_instruction: Optional system prompt
            tools: Optional list of tool definitions
            thinking_level: "minimal", "low", "medium", "high" (default: high)
            
        Returns:
            Dict containing:
            - interaction_id: Generated UUID (not a real session ID)
            - response: The model's response text
            - thinking: The model's thinking/reasoning process
            - status: Completion status
            - tokens: Token usage breakdown
        """
        import uuid
        
        # Build generation config with thinking level
        config = types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(
                thinking_level=thinking_level
            )
        )
        
        # Add system instruction if provided
        if system_instruction:
            config.system_instruction = system_instruction
        
        # Add tools if provided
        if tools:
            config.tools = tools
        
        try:
            # Run in thread pool since google-genai is sync
            loop = asyncio.get_event_loop()
            
            response = await loop.run_in_executor(
                None,
                lambda: self.client.models.generate_content(
                    model=model,
                    contents=prompt,
                    config=config,
                )
            )
            
            # Extract response text
            response_text = response.text if hasattr(response, 'text') else ""
            
            # Extract thinking from candidates if available
            thinking_text = ""
            if hasattr(response, 'candidates') and response.candidates:
                for candidate in response.candidates:
                    if hasattr(candidate, 'content') and candidate.content:
                        for part in candidate.content.parts:
                            if hasattr(part, 'thought') and part.thought:
                                thinking_text += str(part.thought)
            
            # Extract token usage
            token_usage = self._extract_token_usage(response)
            
            return {
                "interaction_id": str(uuid.uuid4()),  # Not a real session ID
                "response": response_text,
                "thinking": thinking_text,  # Full thinking available!
                "status": "completed",
                "tokens": token_usage,
                "raw": response,
            }
            
        except Exception as e:
            raise Exception(f"Failed to generate with thinking: {str(e)}")
    
    async def generate_chat_with_thinking(
        self,
        history: List[Dict[str, str]],
        system_instruction: Optional[str] = None,
        model: str = "gemini-3-flash-preview",
        thinking_level: str = "low",
    ) -> Dict[str, Any]:
        """
        Generate a response with thinking mode using CLIENT-SIDE history management.
        
        USE THIS FOR: Multi-turn conversations where you need both:
                      - Full conversation context (history)
                      - Thinking/reasoning traces for observability
        
        NOTE: This uses models.generate_content (NOT Interactions API) because
              the beta Interactions API does NOT support thinking_config.
              Client-side history means we send the full conversation each time,
              which uses more tokens but unlocks thinking observability.
        
        Args:
            history: List of message dicts with "role" and "content" keys.
                     Roles should be "user" or "model" (Google's naming).
            system_instruction: Optional system prompt.
            model: Which Gemini model to use.
            thinking_level: "minimal", "low", "medium", "high" (default: low).
            
        Returns:
            Dict containing:
            - response: The model's response text
            - thinking: The model's thinking/reasoning process
            - tokens: Token usage breakdown
        """
        import uuid
        
        # Build contents from history
        contents = []
        for msg in history:
            role = msg.get("role", "user")
            # Map roles: "user" stays "user", "assistant"/"model" -> "model"
            if role in ["assistant", "model"]:
                role = "model"
            elif role == "system":
                continue  # Skip system messages in content, they go in system_instruction
            
            contents.append(types.Content(
                role=role,
                parts=[types.Part(text=msg.get("content", ""))]
            ))
        
        # Build generation config with thinking level
        config = types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(
                thinking_level=thinking_level
            )
        )
        
        # Add system instruction if provided
        if system_instruction:
            config.system_instruction = system_instruction
        
        try:
            # Run in thread pool since google-genai is sync
            loop = asyncio.get_event_loop()
            
            response = await loop.run_in_executor(
                None,
                lambda: self.client.models.generate_content(
                    model=model,
                    contents=contents,
                    config=config,
                )
            )
            
            # Extract response text
            response_text = response.text if hasattr(response, 'text') else ""
            
            # Extract thinking from candidates if available
            thinking_text = ""
            if hasattr(response, 'candidates') and response.candidates:
                for candidate in response.candidates:
                    if hasattr(candidate, 'content') and candidate.content:
                        for part in candidate.content.parts:
                            if hasattr(part, 'thought') and part.thought:
                                thinking_text += str(part.thought)
            
            # Extract token usage
            token_usage = self._extract_token_usage(response)
            
            return {
                "interaction_id": str(uuid.uuid4()),  # Not a real session ID
                "response": response_text,
                "thinking": thinking_text,  # Full thinking available!
                "status": "completed",
                "tokens": token_usage,
                "raw": response,
            }
            
        except Exception as e:
            raise Exception(f"Failed to generate chat with thinking: {str(e)}")

    async def generate_with_thinking_stream(
        self,
        prompt: Union[str, List[Dict]],
        model: str = "gemini-3-flash-preview",
        system_instruction: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        thinking_level: str = "high",
    ):
        """
        Generate content with thinking mode enabled, yielding chunks.
        
        Args:
            prompt: Input prompt
            model: Model to use
            system_instruction: Optional system prompt
            tools: Optional tools
            thinking_level: Thinking configuration
            
        Yields:
            Dict with one of:
            - {"type": "thinking", "text": "..."}  - Thinking chunk
            - {"type": "content", "text": "..."}   - Content chunk
            - {"type": "done", "tokens": {...}}    - Final token counts
        """
        import asyncio
        from google import genai
        from google.genai import types

        config = types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(
                thinking_level=thinking_level
            )
        )
        
        if system_instruction:
            config.system_instruction = system_instruction
            
        if tools:
            config.tools = tools
            
        loop = asyncio.get_event_loop()
        
        # Track token usage - will be extracted from final chunk
        token_usage = {
            "input_tokens": 0,
            "output_tokens": 0,
            "thinking_tokens": 0,
            "cached_tokens": 0,
            "total_tokens": 0,
        }
        
        try:
            stream = await loop.run_in_executor(
                None,
                lambda: self.client.models.generate_content_stream(
                    model=model,
                    contents=prompt,
                    config=config,
                )
            )
            
            for chunk in stream:
                # Check for thinking and content parts
                if hasattr(chunk, 'candidates') and chunk.candidates:
                    for candidate in chunk.candidates:
                        if hasattr(candidate, 'content') and candidate.content:
                            for part in candidate.content.parts:
                                if hasattr(part, 'thought') and part.thought:
                                    yield {"type": "thinking", "text": str(part.thought)}
                                elif hasattr(part, 'text') and part.text:
                                    yield {"type": "content", "text": str(part.text)}
                
                # Extract token usage from usage_metadata (last chunk has final counts)
                if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                    meta = chunk.usage_metadata
                    token_usage["input_tokens"] = getattr(meta, 'prompt_token_count', 0) or 0
                    token_usage["output_tokens"] = getattr(meta, 'candidates_token_count', 0) or 0
                    token_usage["thinking_tokens"] = getattr(meta, 'thoughts_token_count', 0) or 0
                    token_usage["cached_tokens"] = getattr(meta, 'cached_content_token_count', 0) or 0
                    token_usage["total_tokens"] = (
                        token_usage["input_tokens"] + 
                        token_usage["output_tokens"] + 
                        token_usage["thinking_tokens"]
                    )
            
            # Yield final token counts
            yield {"type": "done", "tokens": token_usage}
                                    
        except Exception as e:
            print(f"Streaming error: {e}")
            raise


    async def generate_chat_with_thinking_stream(
        self,
        history: List[Dict[str, str]],
        system_instruction: Optional[str] = None,
        model: str = "gemini-3-flash-preview",
        thinking_level: str = "low",
    ):
        """
        Streams SEE-formatted events for chat (thinking + content).
        """
        # ... existing implementation ...
        import asyncio
        from google import genai
        from google.genai import types
        import json
        
        # Build contents from history
        contents = []
        for msg in history:
            role = msg.get("role", "user")
            if role in ["assistant", "model"]:
                role = "model"
            elif role == "system":
                continue
            
            contents.append(types.Content(
                role=role,
                parts=[types.Part(text=msg.get("content", ""))]
            ))
            
        config = types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(
                thinking_level=thinking_level
            )
        )
        if system_instruction:
            config.system_instruction = system_instruction

        loop = asyncio.get_event_loop()
        
        try:
            stream = await loop.run_in_executor(
                None,
                lambda: self.client.models.generate_content_stream(
                    model=model,
                    contents=contents,
                    config=config,
                )
            )
            
            for chunk in stream:
                if hasattr(chunk, 'candidates') and chunk.candidates:
                    for candidate in chunk.candidates:
                        if hasattr(candidate, 'content') and candidate.content:
                            for part in candidate.content.parts:
                                if hasattr(part, 'thought') and part.thought:
                                    yield f"event: thinking\ndata: {json.dumps({'text': str(part.thought)})}\n\n"
                                elif hasattr(part, 'text') and part.text:
                                    yield f"event: content\ndata: {json.dumps({'text': str(part.text)})}\n\n"
            
            # Send done event (simplified, no stats for stream)
            yield f"event: done\ndata: {json.dumps({'content': '', 'thinking': ''})}\n\n"
                                    
        except Exception as e:
            print(f"Streaming error: {e}")
            raise
        """
        Stream a response with thinking mode using CLIENT-SIDE history management.
        
        Yields SSE-formatted strings for real-time UI updates:
        - event: thinking\ndata: <chunk>\n\n
        - event: content\ndata: <chunk>\n\n
        - event: done\ndata: {tokens: {...}}\n\n
        
        NOTE: This uses models.generate_content_stream for true streaming.
        
        Args:
            history: List of message dicts with "role" and "content" keys.
            system_instruction: Optional system prompt.
            model: Which Gemini model to use.
            thinking_level: "minimal", "low", "medium", "high".
            
        Yields:
            SSE-formatted strings (event: <type>\ndata: <payload>\n\n)
        """
        import json
        
        # Build contents from history
        contents = []
        for msg in history:
            role = msg.get("role", "user")
            if role in ["assistant", "model"]:
                role = "model"
            elif role == "system":
                continue
            
            contents.append(types.Content(
                role=role,
                parts=[types.Part(text=msg.get("content", ""))]
            ))
        
        # Build generation config with thinking level
        config = types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(
                thinking_level=thinking_level
            )
        )
        
        if system_instruction:
            config.system_instruction = system_instruction
        
        try:
            # google-genai SDK streaming is sync, so we run in executor but iterate
            loop = asyncio.get_event_loop()
            
            # Get the stream response iterator
            stream = await loop.run_in_executor(
                None,
                lambda: self.client.models.generate_content_stream(
                    model=model,
                    contents=contents,
                    config=config,
                )
            )
            
            accumulated_thinking = ""
            accumulated_content = ""
            token_usage = {}
            
            # Iterate over streamed chunks
            for chunk in stream:
                # Extract thinking parts first
                if hasattr(chunk, 'candidates') and chunk.candidates:
                    for candidate in chunk.candidates:
                        if hasattr(candidate, 'content') and candidate.content:
                            for part in candidate.content.parts:
                                if hasattr(part, 'thought') and part.thought:
                                    thinking_chunk = str(part.thought)
                                    accumulated_thinking += thinking_chunk
                                    yield f"event: thinking\ndata: {json.dumps({'text': thinking_chunk})}\n\n"
                                    await asyncio.sleep(0)  # Yield control
                                elif hasattr(part, 'text') and part.text:
                                    content_chunk = part.text
                                    accumulated_content += content_chunk
                                    yield f"event: content\ndata: {json.dumps({'text': content_chunk})}\n\n"
                                    await asyncio.sleep(0)  # Yield control
                
                # Extract token usage from final chunk
                if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                    token_usage = self._extract_token_usage(chunk)
            
            # Final done event with accumulated content and usage
            yield f"event: done\ndata: {json.dumps({'thinking': accumulated_thinking, 'content': accumulated_content, 'tokens': token_usage})}\n\n"
            
        except Exception as e:
            yield f"event: error\ndata: {json.dumps({'error': str(e)})}\n\n"
    
    async def start_deep_research(
        self,
        topic: str,
        agent: str = "deep-research-pro-preview-12-2025",
    ) -> Dict[str, Any]:
        """
        Start an asynchronous Deep Research task.
        
        Deep Research runs in the background and can take 20-60 minutes.
        Use poll_research_status() to check for completion.
        
        Args:
            topic: The research topic/question
            agent: The research agent identifier
            
        Returns:
            Dict containing:
            - task_id: The background task ID
            - status: Initial status ("in_progress")
        """
        try:
            loop = asyncio.get_event_loop()
            interaction = await loop.run_in_executor(
                None,
                lambda: self.client.interactions.create(
                    agent=agent,
                    input=f"Research: {topic}",
                    background=True,
                    config=types.InteractionConfig(store=True),
                )
            )
            
            return {
                "task_id": interaction.id,
                "status": interaction.status if hasattr(interaction, 'status') else "in_progress",
                "message": f"Started research on: {topic}",
            }
            
        except Exception as e:
            raise Exception(f"Failed to start deep research: {str(e)}")
    
    async def poll_research_status(
        self,
        task_id: str,
        max_wait_seconds: int = 3600,
        poll_interval: int = 10,
    ) -> Dict[str, Any]:
        """
        Poll for Deep Research completion with exponential backoff.
        
        Args:
            task_id: The research task ID
            max_wait_seconds: Maximum time to wait
            poll_interval: Initial polling interval in seconds
            
        Returns:
            Dict containing:
            - status: "completed", "failed", or "in_progress"
            - result: The research output (if completed)
        """
        elapsed = 0
        current_interval = poll_interval
        max_interval = 60
        
        while elapsed < max_wait_seconds:
            try:
                loop = asyncio.get_event_loop()
                current_state = await loop.run_in_executor(
                    None,
                    lambda: self.client.interactions.get(id=task_id)
                )
                
                status = current_state.status if hasattr(current_state, 'status') else "unknown"
                
                if status == "completed":
                    result_text = ""
                    if current_state.outputs:
                        for output in current_state.outputs:
                            if hasattr(output, 'text'):
                                result_text += output.text
                    return {
                        "status": "completed",
                        "result": result_text,
                    }
                    
                elif status == "failed":
                    return {
                        "status": "failed",
                        "result": None,
                        "error": getattr(current_state, 'error', 'Unknown error'),
                    }
                    
                elif status == "cancelled":
                    return {
                        "status": "cancelled",
                        "result": None,
                    }
                    
            except Exception as e:
                print(f"Polling error: {e}")
            
            # Wait and increment interval
            await asyncio.sleep(current_interval)
            elapsed += current_interval
            current_interval = min(current_interval + 5, max_interval)
        
        return {
            "status": "timeout",
            "result": None,
        }
    
    async def process_document(
        self,
        file_path: str,
        prompt: str = "Analyze this document for key presentation points.",
        model: str = "gemini-3-flash-preview",
        thinking_level: str = "low",
    ) -> Dict[str, Any]:
        """
        Process a document (PDF, image, etc.) using Gemini's native multimodal.
        
        Gemini 3 Flash supports native PDF processing without external libraries.
        
        Args:
            file_path: Path to the document file
            prompt: The analysis prompt
            model: Model to use (Flash recommended for speed)
            thinking_level: Reasoning depth
            
        Returns:
            Dict with interaction_id and response
        """
        path = Path(file_path)
        
        # Determine MIME type
        mime_types = {
            ".pdf": "application/pdf",
            ".png": "image/png",
            ".jpg": "image/jpeg",
            ".jpeg": "image/jpeg",
            ".gif": "image/gif",
            ".webp": "image/webp",
        }
        mime_type = mime_types.get(path.suffix.lower(), "application/octet-stream")
        
        # Read and encode file
        with open(file_path, "rb") as f:
            file_data = base64.standard_b64encode(f.read()).decode("utf-8")
        
        # Build multimodal input
        multimodal_input = [
            {"text": prompt},
            {"inline_data": {"mime_type": mime_type, "data": file_data}}
        ]
        
        return await self.create_interaction(
            prompt=multimodal_input,
            model=model,
            thinking_level=thinking_level,
        )
    
    def _build_part(self, part: Dict) -> types.Part:
        """Build a Part object from a dict."""
        if "text" in part:
            return types.Part(text=part["text"])
        elif "inline_data" in part:
            return types.Part(
                inline_data=types.Blob(
                    mime_type=part["inline_data"]["mime_type"],
                    data=part["inline_data"]["data"],
                )
            )
        else:
            raise ValueError(f"Unknown part type: {part}")


class GeminiInteractionsLLM:
    """
    CrewAI-compatible LLM wrapper for Gemini Interactions API.
    
    This class can be used with CrewAI agents while maintaining
    stateful conversations via the Interactions API.
    
    Usage:
        llm = GeminiInteractionsLLM(api_key="...", model="gemini-3-flash-preview")
        agent = Agent(llm=llm, ...)
    """
    
    def __init__(
        self,
        api_key: str,
        model: str = "gemini-3-flash-preview",
        thinking_level: str = "low",
    ):
        self.api_key = api_key
        self.model = model
        self.thinking_level = thinking_level
        self.client = GeminiInteractionsClient(api_key)
        self.interaction_id: Optional[str] = None
        self.supports_system_prompt = True
    
    def call(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """
        Synchronous call method for CrewAI compatibility.
        
        Note: This wraps the async client for sync usage.
        """
        # Extract the latest message
        if isinstance(messages, list) and messages:
            prompt = messages[-1].get("content", "")
        else:
            prompt = str(messages)
        
        # Run async call in event loop
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If already in async context, use thread
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as pool:
                future = pool.submit(
                    asyncio.run,
                    self._async_call(prompt)
                )
                return future.result()
        else:
            return loop.run_until_complete(self._async_call(prompt))
    
    async def _async_call(self, prompt: str) -> str:
        """Internal async call."""
        if self.interaction_id:
            result = await self.client.continue_interaction(
                interaction_id=self.interaction_id,
                prompt=prompt,
                model=self.model,
                thinking_level=self.thinking_level,
            )
        else:
            result = await self.client.create_interaction(
                prompt=prompt,
                model=self.model,
                thinking_level=self.thinking_level,
            )
        
        self.interaction_id = result.get("interaction_id")
        return result.get("response", "")
    
    def reset_session(self):
        """Reset the interaction session (start fresh)."""
        self.interaction_id = None
